{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from importlib import reload\n",
    "from tqdm.notebook import tqdm\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor\n",
    "from transformers import GPT2Config, GPT2Tokenizer, GPT2LMHeadModel, AutoTokenizer\n",
    "\n",
    "pl.seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## config and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': {'batch_size': 20,\n",
       "  'max_len': 256,\n",
       "  'csv_file': 'data/processed_cleaned.csv',\n",
       "  'tokenizer_name': 'bolbolzaban/gpt2-persian',\n",
       "  'val_frac': 0.1},\n",
       " 'model': {'n_positions': 256,\n",
       "  'n_ctx': 256,\n",
       "  'n_embd': 512,\n",
       "  'n_layer': 4,\n",
       "  'n_head': 8,\n",
       "  'n_inner': 768},\n",
       " 'opt': {'lr': 5e-05, 'max_epochs': 10, 'weight_decay': 0.001}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "config = yaml.load(open('configs/config.yaml'), Loader=yaml.Loader)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(config['data']['tokenizer_name'])\n",
    "# tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': {'batch_size': 20,\n",
       "  'max_len': 256,\n",
       "  'csv_file': 'data/processed_cleaned.csv',\n",
       "  'tokenizer_name': 'bolbolzaban/gpt2-persian',\n",
       "  'val_frac': 0.1},\n",
       " 'model': {'n_positions': 256,\n",
       "  'n_ctx': 256,\n",
       "  'n_embd': 512,\n",
       "  'n_layer': 4,\n",
       "  'n_head': 8,\n",
       "  'n_inner': 768,\n",
       "  'vocab_size': 25000,\n",
       "  'pad_token_id': 6,\n",
       "  'eos_token_id': 9,\n",
       "  'bos_token_id': 8},\n",
       " 'opt': {'lr': 5e-05, 'max_epochs': 10, 'weight_decay': 0.001}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config['model']['vocab_size'] = tokenizer.vocab_size\n",
    "config['model']['pad_token_id'] = tokenizer.pad_token_id\n",
    "config['model']['eos_token_id'] = tokenizer.eos_token_id\n",
    "config['model']['bos_token_id'] = tokenizer.bos_token_id\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(config['data']['csv_file'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de378fee92c44f3f907bf6ad10a888e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=91824.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "contexted = []\n",
    "n = 7\n",
    "for i in tqdm(range(n, len(df['text']))):\n",
    "    row = []\n",
    "    prev = i - 1 - n # we additionally substract 1, so row will contain current responce and 7 previous responces  \n",
    "    for j in range(i, prev, -1):\n",
    "        row.append(df['text'][j])\n",
    "    contexted.append(row)  \n",
    "    \n",
    "columns = ['response', 'context'] \n",
    "columns = columns + ['context/'+str(i) for i in range(n-1)]\n",
    "df = pd.DataFrame.from_records(contexted, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset has 82642 samples and val dataset has 9182 samples\n"
     ]
    }
   ],
   "source": [
    "import src.dataset\n",
    "reload(src.dataset)\n",
    "from src.dataset import get_dataloaders\n",
    "\n",
    "train_loader, val_loader = get_dataloaders(tokenizer, df, \n",
    "                                           max_len=config['data']['max_len'], \n",
    "                                           batch_size=config['data']['batch_size'], \n",
    "                                           val_frac=config['data']['val_frac'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids torch.Size([20, 256])\n",
      "token_type_ids torch.Size([20, 256])\n",
      "attention_mask torch.Size([20, 256])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    for k,v in batch.items():\n",
    "        print(k,v.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ErfBot(\n",
       "  (model): GPT2LMHeadModel(\n",
       "    (transformer): GPT2Model(\n",
       "      (wte): Embedding(25000, 512)\n",
       "      (wpe): Embedding(256, 512)\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (h): ModuleList(\n",
       "        (0): Block(\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Block(\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): Block(\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): Block(\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=512, out_features=25000, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import src.model\n",
    "reload(src.model)\n",
    "from src.model import ErfBot\n",
    "\n",
    "\n",
    "model = ErfBot(config=GPT2Config(**config['model']), \n",
    "               **config['opt'], \n",
    "               use_pretrained=False)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ErfBot.load_from_checkpoint('weights/gpt2/model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20293632"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.count_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/soroosh/.local/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Checkpoint directory weights/gpt2 exists and is not empty.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "GPU available: True, used: True\n",
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "TPU available: None, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    }
   ],
   "source": [
    "logger = TensorBoardLogger(\n",
    "    save_dir='logs/',\n",
    "    name='gpt2_logs'\n",
    ")\n",
    "\n",
    "checkpoint = ModelCheckpoint(dirpath='weights/gpt2', \n",
    "                             filename='{epoch}-{val_loss:.2f}', \n",
    "                             monitor='val_loss',\n",
    "                             save_top_k=1, \n",
    "                             period=1)\n",
    "\n",
    "lr_logger = LearningRateMonitor(logging_interval='step')\n",
    "\n",
    "## defining trainer\n",
    "trainer = pl.Trainer(benchmark=True, \n",
    "                  gpus=1, \n",
    "                  logger=logger, \n",
    "                  max_epochs=config['opt']['max_epochs'],\n",
    "                  callbacks=[checkpoint, lr_logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name  | Type            | Params\n",
      "------------------------------------------\n",
      "0 | model | GPT2LMHeadModel | 20.3 M\n",
      "------------------------------------------\n",
      "20.3 M    Trainable params\n",
      "0         Non-trainable params\n",
      "20.3 M    Total params\n",
      "\n",
      "  | Name  | Type            | Params\n",
      "------------------------------------------\n",
      "0 | model | GPT2LMHeadModel | 20.3 M\n",
      "------------------------------------------\n",
      "20.3 M    Trainable params\n",
      "0         Non-trainable params\n",
      "20.3 M    Total params\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d42224f902c441c9f425878b534d059",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_checkpoint('weights/gpt2/model.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in range(10):\n",
    "    new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n",
    "\n",
    "    bot_input_ids = torch.cat([chat_history_ids.cuda(), new_user_input_ids.cuda()], dim=-1) if step > 0 else new_user_input_ids.cuda()\n",
    "\n",
    "    chat_history_ids = model.model.generate(bot_input_ids, \n",
    "                                            max_length=1000, \n",
    "                                            top_p=0.9, \n",
    "                                            num_beams=8,\n",
    "                                            do_sample=True, \n",
    "                                            temperature=0.8,\n",
    "                                            pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    # pretty print last ouput tokens from bot\n",
    "    print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
