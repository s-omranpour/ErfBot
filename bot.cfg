[pipeline]
# The model that will be used by the pipeline to make predictions.
# The model must be an instance of a pretrained model inheriting from PreTrainedModel or TFPreTrainedModel.
model = ''

# The framework to use, either pt for PyTorch or tf for TensorFlow.
# The specified framework must be installed.
framework = 'pt' 

# Device ordinal for CPU/GPU supports.
# Setting this to -1 will leverage CPU, a positive will run the model on the associated CUDA device id.
device = -1

[generator]
# See https://huggingface.co/blog/how-to-generate

# Seed for random number generators, fix seed to reproduce results.
# By default there is no seed and each turn should be unique.
seed = 42

# The maximal number of tokens to be returned, inclusive of punctuations etc.
# It will automatically stop if the end-of-sequence token was found earlier.
max_length = 128

# The minimum length of the sequence to be generated.
min_length = 1

# Whether or not to use sampling; use greedy decoding otherwise.
do_sample = True

# Whether to stop the beam search when at least num_beams sentences are finished per batch or not.
early_stopping = False

# Number of beams for beam search. 1 means no beam search.
num_beams = 1

# The value used to module the next token probabilities.
# Lower temperature results in less random completions.
# As the temperature approaches zero, the model will become deterministic and repetitive.
# Higher temperature results in more random completions.
temperature = 0.7

# The number of highest probability vocabulary tokens to keep for top-k-filtering.
# 1 means only 1 word is considered for each step (token), resulting in deterministic completions,
# while 40 means 40 words are considered at each step.
# 0 (default) is a special setting meaning no restrictions.
# 40 generally is a good value.
top_k = 40

# If set to float < 1, only the most probable tokens with probabilities
# that add up to top_p or higher are kept for generation.
top_p = 1.0

# The parameter for repetition penalty. 1.0 means no penalty.
repetition_penalty = 1.0

# The id of the padding token.
pad_token_id

# The id of the beginning-of-sequence token.
bos_token_id

# The id of the end-of-sequence token.
eos_token_id

# Exponential penalty to the length. 1.0 means no penalty.
# Set to values < 1.0 in order to encourage the model to generate shorter sequences,
# to a value > 1.0 in order to encourage the model to produce longer sequences.
length_penalty = 1.1

# If set to int > 0, all ngrams of that size can only occur once.
no_repeat_ngram_size = 0

num_return_sequences = 1


# Whether or not the model should use the past last key/values attentions
# (if applicable to the model) to speed up decoding.
use_cache = True

# Whether or not to clean up the potential extra spaces in the text output.
clean_up_tokenization_spaces = True

[chatbot]

# The number of turns (turn = answer and response) the model should consider.
# Set to 0 to focus on the last message. Set to -1 for unlimited context length.
max_turns_history = 2

# Your Telegram token. See https://core.telegram.org/bots.
telegram_token = YOUR_TOKEN_HERE